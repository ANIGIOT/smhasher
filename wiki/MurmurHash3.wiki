#summary MurmurHash3 information and brief performance results

= Introduction =

MurmurHash3 is the successor to MurmurHash2. It comes in 4 variants - 32/64/128-bit version for 32-bit platforms, and a 128-bit version for 64-bit platforms. The 32- and 64-bit versions for 64-bit platforms just use a subset of the 128-bit result - this was faster than trying to make specialized versions for each hash size.

== Details ==

MurmurHash3's mix functions are based on this snippet - 

{{{
k1 *= c1; 
k1  = _rotl(k1,11); 
k1 *= c2;

h1 ^= k1;
	
h1 = h1*3+0x52dce729;

c1 = c1*5+0x7b7d159c;
c2 = c2*5+0x6bce6396;
}}}

'k' is a block of the key, 'h' is a block of the hash state, and 'c1' and 'c2' are two dynamic mixing constants.

For each block of the key, we pre-mix it using the two constants and a rotate, xor it into the hash block, mix the hash block, and then mix the two constants we used to mix the key block.

The changing mix constants help ensure that each key block is mixed in a slightly different way, which serves as a 'pattern breaker' - repeated key blocks don't all affect the hash value in the same way, which makes it much less likely that small changes in those blocks can cancel each other out. This improves collision resistance by a significant factor, and makes an otherwise weak-but-fast mix function quite robust.

The 'x = x*n+c' operations are also weak, mixing-wise, but a modern optimizing x86/x64 compiler will convert them into a single 'lea' instruction if the 'n' value is 3, 5, or 9 and the 'c' value is less than 0x8000000. The 'lea' instruction can be evaluated in the CPU pipeline in parallel with some of the multiply and rotate instructions, which makes for a very very fast overall mix.

One downside of the weak-ish mix and the constantly changing 'c1' and 'c2' constants is that it is hard to guarantee that a particular block of the key will be mixed sufficiently before it leaves the hash function - to ensure that the output of the hash function always [Avalanche avalanches], MurmurHash3 adds a stronger finalization step that guarantees that all bits of the input affect all bits of the output equally to within a very small percentage of ideal randomness.


== Bulk speed test, hashing an 8-byte-aligned 256k block ==

{{{
  * FNV_x86_32           -  554 mb/sec 
  * FNV_x64_32           -  715 mb/sec
  * SuperFastHash_x86_32 - 1224 mb/sec (1) 
  * SuperFastHash_x64_32 - 1311 mb/sec
  * Lookup3_x86_32       - 1234 mb/sec 
  * Lookup3_x64_32       - 1265 mb/sec
  * MurmurHash2_x86_32   - 2577 mb/sec
  * MurmurHash2_x86_64   - 3352 mb/sec (2)
  * MurmurHash2_x64_64   - 2857 mb/sec
  * MurmurHash3_x86_32   - 3105 mb/sec 
  * MurmurHash3_x86_64   - 2901 mb/sec 
  * MurmurHash3_x86_128  - 2803 mb/sec
  * MurmurHash3_x64_128  - *5058* mb/sec (3)
}}}

(1) - SuperFastHash has very poor collision properties, which have been documented elsewhere.

(2) - MurmurHash2_x86_64 computes two 32-bit results in parallel and mixes them at the end, which is fast but means that collision resistance is only as good as a 32-bit hash. I suggest avoiding this variant.

(3) - Yes, 5 gigabytes per second. That's about 1.68 bytes per cycle, or about 9.5 cycles per 16-byte chunk. The inner loop is 20 instructions long, so we're sustaining almost 2 instructions per cycle. Hooray for modern platforms with fast 64-bit multipliers and superscalar architectures. :)